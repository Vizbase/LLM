{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40229ab-95bd-4220-a328-7b0e1dc16bb8",
   "metadata": {},
   "source": [
    "# Tokenization and Vocabulary Building for Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276c400-f3ac-483b-9699-7dc31c6cd479",
   "metadata": {},
   "source": [
    "## Reading and Loading a Text File\n",
    "This code block reads a text file into Python, displaying the total number of characters and a preview of the text for context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ae809-d33f-4215-bb2b-159444da26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])  # Displaying the first 100 characters for context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fd5c7-c7a0-4b37-a4c2-927d7ac677a2",
   "metadata": {},
   "source": [
    "## Tokenizing Text\n",
    "Here, we tokenize the input text into words and punctuation using regular expressions, preparing it for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5896b4-28a9-445d-88c7-27eecf7c68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenize the text using regular expressions\n",
    "tokens = re.split(r'[\\s,.;:?!]+', raw_text)\n",
    "print(\"Sample Tokens:\", tokens[:10])  # Displaying the first 10 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173166a-e8fd-4b66-89c3-6450757e067d",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary\n",
    "This step involves creating a vocabulary that maps unique tokens to numerical indices, enabling efficient text encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a4f80-39fc-4c89-8b95-1334957953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary of unique tokens\n",
    "unique_tokens = sorted(set(tokens))\n",
    "vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"Sample Vocabulary Entries:\", list(vocab.items())[:10])  # Displaying first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e28670-8249-4a1d-a3e8-d955b07470c5",
   "metadata": {},
   "source": [
    "## Mapping Tokens to Token IDs\n",
    "This block maps each token in the text to its corresponding numerical ID based on the created vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa1f4c-ac85-4b51-98c8-aa674f3d786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tokens to token IDs\n",
    "token_ids = [vocab[token] for token in tokens if token in vocab]\n",
    "print(\"Sample Token IDs:\", token_ids[:10])  # Displaying the first 10 token IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaa294-1d8a-4003-aa92-ca22fef5317c",
   "metadata": {},
   "source": [
    "## Implementing a Simple Tokenizer Class\n",
    "This class-based tokenizer encodes text into token IDs and decodes token IDs back into text, showcasing bidirectional functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe0db4-c3fc-4c3f-aac7-9454bb4a5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'[\\s,.;:?!]+', text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab.get(\"[UNK]\")) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.reverse_vocab.get(token_id, \"[UNK]\") for token_id in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# Instantiate and test the tokenizer\n",
    "tokenizer = Tokenizer(vocab)\n",
    "sample_text = \"This is a test.\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64364f-e39e-4eef-b510-3cebbfe90e9e",
   "metadata": {},
   "source": [
    "## Handling Out-of-Vocabulary (OOV) Tokens\n",
    "This code demonstrates handling unknown words by replacing them with a special `[UNK]` token during encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061d8f8-7468-471b-b8c4-c9af87964485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWithOOV(Tokenizer):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(vocab)\n",
    "        self.vocab[\"[UNK]\"] = max(self.vocab.values()) + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'[\\s,.;:?!]+', text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "# Test the tokenizer with OOV handling\n",
    "tokenizer_oov = TokenizerWithOOV(vocab)\n",
    "sample_text = \"An unknown word test.\"\n",
    "encoded = tokenizer_oov.encode(sample_text)\n",
    "decoded = tokenizer_oov.decode(encoded)\n",
    "\n",
    "print(\"Encoded with OOV:\", encoded)\n",
    "print(\"Decoded with OOV:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38744d4c-54bd-4aec-99c5-6eb6b5522b27",
   "metadata": {},
   "source": [
    "## Enhanced Tokenizer with Special Tokens\n",
    "This block adds support for special tokens like `<|unk|>` and `<|endoftext|>`, improving the tokenizer's robustness and functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530de36c-ad19-4df2-aad6-a5170a6b507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "        self.vocab[\"<|unk|>\"] = len(vocab)\n",
    "        self.vocab[\"<|endoftext|>\"] = len(vocab) + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'[\\s,.;:?!]+', text) + [\"<|endoftext|>\"]\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<|unk|>\"]) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.reverse_vocab.get(token_id, \"<|unk|>\") for token_id in token_ids]\n",
    "        return \" \".join(tokens).replace(\" <|endoftext|>\", \"\")\n",
    "\n",
    "# Instantiate and test the enhanced tokenizer\n",
    "enhanced_tokenizer = EnhancedTokenizer(vocab)\n",
    "sample_text = \"A new text example.\"\n",
    "encoded = enhanced_tokenizer.encode(sample_text)\n",
    "decoded = enhanced_tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded with Special Tokens:\", encoded)\n",
    "print(\"Decoded with Special Tokens:\", decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
